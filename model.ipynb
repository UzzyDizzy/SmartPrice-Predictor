{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ec9dd26-7e0a-4f7e-a3c6-f8cd7da0fca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (2.8.0+cu128)\n",
      "Requirement already satisfied: torchvision in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (0.23.0+cu128)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.57.0-py3-none-any.whl.metadata (41 kB)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.10.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting peft\n",
      "  Downloading peft-0.17.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.48.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Collecting open-clip-torch\n",
      "  Downloading open_clip_torch-3.2.0-py3-none-any.whl.metadata (32 kB)\n",
      "Requirement already satisfied: pandas in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (2.1.4)\n",
      "Requirement already satisfied: numpy in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (1.3.2)\n",
      "Requirement already satisfied: Pillow in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (11.3.0)\n",
      "Requirement already satisfied: requests in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (2.32.5)\n",
      "Requirement already satisfied: tqdm in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (4.67.1)\n",
      "Collecting ftfy\n",
      "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting regex\n",
      "  Downloading regex-2025.9.18-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: filelock in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (78.1.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (3.4.0)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers) (6.0.3)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: psutil in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from accelerate) (7.1.0)\n",
      "Collecting timm>=1.0.17 (from open-clip-torch)\n",
      "  Downloading timm-1.0.20-py3-none-any.whl.metadata (61 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests) (2025.10.5)\n",
      "Requirement already satisfied: wcwidth in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from ftfy) (0.2.14)\n",
      "Requirement already satisfied: six>=1.5 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from jinja2->torch) (3.0.3)\n",
      "Downloading transformers-4.57.0-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m244.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m250.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.10.1-py3-none-any.whl (374 kB)\n",
      "Downloading peft-0.17.1-py3-none-any.whl (504 kB)\n",
      "Downloading bitsandbytes-0.48.1-py3-none-manylinux_2_24_x86_64.whl (60.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m233.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading open_clip_torch-3.2.0-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m147.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "Downloading regex-2025.9.18-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (802 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m802.0/802.0 kB\u001b[0m \u001b[31m93.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Downloading timm-1.0.20-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m217.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, regex, hf-xet, ftfy, huggingface-hub, tokenizers, transformers, bitsandbytes, accelerate, timm, peft, open-clip-torch\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/12\u001b[0m [open-clip-torch] [peft]erate]s]ub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed accelerate-1.10.1 bitsandbytes-0.48.1 ftfy-6.3.1 hf-xet-1.1.10 huggingface-hub-0.35.3 open-clip-torch-3.2.0 peft-0.17.1 regex-2025.9.18 safetensors-0.6.2 timm-1.0.20 tokenizers-0.22.1 transformers-4.57.0\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.12.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from tiktoken) (2025.9.18)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from tiktoken) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2025.10.5)\n",
      "Downloading tiktoken-0.12.0-cp312-cp312-manylinux_2_28_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.12.0\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
      "Downloading sentencepiece-0.2.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision transformers accelerate peft bitsandbytes open-clip-torch pandas numpy scikit-learn Pillow requests tqdm ftfy regex\n",
    "\n",
    "!pip install tiktoken\n",
    "\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfe14ff7-9f40-4eee-ad7c-c699336d2ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Loading CSV files...\n",
      "🚀 Downloading 75000 training images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 38882/75000 [00:21<00:19, 1868.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Not able to download - https://m.media-amazon.com/images/I/51mjZYDYjyL.jpg\n",
      "HTTP Error 404: Not Found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75000/75000 [00:40<00:00, 1864.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Downloading 75000 test images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 41896/75000 [00:21<00:16, 2041.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Not able to download - https://m.media-amazon.com/images/I/813CjSgHj0S.jpg\n",
      "HTTP Error 404: Not Found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75000/75000 [00:39<00:00, 1904.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All downloads completed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import time\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "from requests.exceptions import ConnectionError, Timeout, RequestException\n",
    "\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "from time import time as timer\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "import requests\n",
    "import urllib\n",
    "\n",
    "def download_image(image_link, savefolder):\n",
    "    if(isinstance(image_link, str)):\n",
    "        filename = Path(image_link).name\n",
    "        image_save_path = os.path.join(savefolder, filename)\n",
    "        if(not os.path.exists(image_save_path)):\n",
    "            try:\n",
    "                urllib.request.urlretrieve(image_link, image_save_path)    \n",
    "            except Exception as ex:\n",
    "                print('Warning: Not able to download - {}\\n{}'.format(image_link, ex))\n",
    "        else:\n",
    "            return\n",
    "    return\n",
    "\n",
    "def download_images(image_links, download_folder):\n",
    "    if not os.path.exists(download_folder):\n",
    "        os.makedirs(download_folder)\n",
    "    results = []\n",
    "    download_image_partial = partial(download_image, savefolder=download_folder)\n",
    "    with multiprocessing.Pool(100) as pool:\n",
    "        for result in tqdm(pool.imap(download_image_partial, image_links), total=len(image_links)):\n",
    "            results.append(result)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "\n",
    "# === CONFIG ===\n",
    "TRAIN_CSV = \"train.csv\"\n",
    "TEST_CSV = \"test.csv\"\n",
    "IMG_FOLDER = \"images\"\n",
    "IMAGE_COLUMN = \"image_link\"  # Change if needed (e.g., 'image_url')\n",
    "\n",
    "# === LOAD DATA ===\n",
    "print(\"📥 Loading CSV files...\")\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "test_df = pd.read_csv(TEST_CSV)\n",
    "\n",
    "# === EXTRACT LINKS ===\n",
    "train_links = train_df[IMAGE_COLUMN].dropna().tolist()\n",
    "test_links = test_df[IMAGE_COLUMN].dropna().tolist()\n",
    "\n",
    "# === DOWNLOAD TRAIN IMAGES ===\n",
    "print(f\"🚀 Downloading {len(train_links)} training images...\")\n",
    "download_images(train_links, IMG_FOLDER)\n",
    "\n",
    "# === DOWNLOAD TEST IMAGES ===\n",
    "print(f\"🚀 Downloading {len(test_links)} test images...\")\n",
    "download_images(test_links, IMG_FOLDER)\n",
    "\n",
    "print(\"✅ All downloads completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25bfa7fc-a5dc-4289-81b0-1d5915f6135f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.py\n",
    "from pathlib import Path\n",
    "\n",
    "class Config:\n",
    "    # ===== Model Config =====\n",
    "    text_model_name = \"microsoft/deberta-v3-large\"\n",
    "    vision_model_name = \"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\"\n",
    "    fusion_hidden_dim = 2048\n",
    "    dropout = 0.2\n",
    "\n",
    "    # ===== PEFT / LoRA =====\n",
    "    lora_r = 16\n",
    "    lora_alpha = 32\n",
    "    lora_dropout = 0.1\n",
    "\n",
    "    # ===== Quantization =====\n",
    "    use_8bit = False   # Set to False if using fp32\n",
    "    use_4bit = False   # Set to False if using fp32\n",
    "    use_fp16 = False\n",
    "    use_fp32 = True # Enable fp32 precision\n",
    "\n",
    "\n",
    "    # ===== Training =====\n",
    "    batch_size = 32\n",
    "    learning_rate = 2e-4\n",
    "    num_epochs = 10\n",
    "    warmup_ratio = 0.1\n",
    "    weight_decay = 0.01\n",
    "    max_grad_norm = 1.0\n",
    "    accumulation_steps = 32\n",
    "\n",
    "    # ===== Data =====\n",
    "    max_text_length = 512\n",
    "    image_size = 224\n",
    "\n",
    "    # ===== Paths =====\n",
    "    base_dir = Path(\"./\")\n",
    "    train_csv = Path(\"./train.csv\")\n",
    "    test_csv = Path(\"./test.csv\")\n",
    "    image_folder = Path(\"./images\")\n",
    "\n",
    "    # ===== System =====\n",
    "    num_workers = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fe25455-bb7d-41a7-bd78-0ec4652f098f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.py\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, os, requests\n",
    "from pathlib import Path\n",
    "from transformers import DebertaV2Tokenizer\n",
    "\n",
    "# -----------------------------\n",
    "# Text Feature Extraction\n",
    "# -----------------------------\n",
    "def extract_advanced_features(text: str):\n",
    "    \"\"\"Extract structured information and create enhanced text.\"\"\"\n",
    "    features = {}\n",
    "\n",
    "    # Basic extractions\n",
    "    item_name_match = re.search(r'Item Name:\\s*(.*?)(?:\\n|$)', text)\n",
    "    value_match = re.search(r'Value:\\s*([0-9.]+)', text)\n",
    "    unit_match = re.search(r'Unit:\\s*(.*?)(?:\\n|$)', text)\n",
    "\n",
    "    features['item_name'] = item_name_match.group(1).strip() if item_name_match else \"\"\n",
    "    features['value'] = float(value_match.group(1)) if value_match else 1.0\n",
    "    features['unit'] = unit_match.group(1).strip() if unit_match else \"\"\n",
    "\n",
    "    # Bullet points\n",
    "    bullet_points = re.findall(r'Bullet Point \\d+:\\s*(.*?)(?:\\n|$)', text)\n",
    "    features['bullet_points'] = bullet_points\n",
    "    features['num_bullet_points'] = len(bullet_points)\n",
    "\n",
    "    # Description\n",
    "    desc_match = re.search(r'Product Description:\\s*(.*?)(?:\\nValue:|$)', text, re.DOTALL)\n",
    "    features['description'] = desc_match.group(1).strip() if desc_match else \"\"\n",
    "\n",
    "    # Enhanced text construction\n",
    "    enhanced_text = f\"Product: {features['item_name']}\"\n",
    "    if bullet_points:\n",
    "        enhanced_text += f\". Key features: {' '.join(bullet_points[:3])}\"\n",
    "    if features['description']:\n",
    "        desc_preview = features['description'][:100] + \"...\" if len(features['description']) > 100 else features['description']\n",
    "        enhanced_text += f\". Description: {desc_preview}\"\n",
    "    if features['value'] > 0:\n",
    "        enhanced_text += f\". Package contains: {features['value']} {features['unit']}\"\n",
    "\n",
    "    # Additional features\n",
    "    features['text_length'] = len(text)\n",
    "    features['has_bullet_points'] = len(bullet_points) > 0\n",
    "    features['has_description'] = len(features['description']) > 0\n",
    "\n",
    "    return enhanced_text, features\n",
    "\n",
    "# -----------------------------\n",
    "# Image Download with Retry\n",
    "# -----------------------------\n",
    "def download_image_with_retry(url, save_path, max_retries=3):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            if os.path.exists(save_path):\n",
    "                return True\n",
    "            response = requests.get(url, timeout=10, stream=True)\n",
    "            if response.status_code == 200:\n",
    "                with open(save_path, 'wb') as f:\n",
    "                    for chunk in response.iter_content(chunk_size=8192):\n",
    "                        f.write(chunk)\n",
    "                return True\n",
    "        except Exception as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                print(f\"❌ Failed to download {url}: {e}\")\n",
    "    return False\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset Class\n",
    "# -----------------------------\n",
    "class AdvancedProductDataset(Dataset):\n",
    "    def __init__(self, dataframe: pd.DataFrame, is_training=True):\n",
    "        self.dataframe = dataframe.reset_index(drop=True)\n",
    "        self.is_training = is_training\n",
    "\n",
    "        # Tokenizer\n",
    "        self.text_tokenizer = DebertaV2Tokenizer.from_pretrained(Config.text_model_name)\n",
    "        if self.text_tokenizer.pad_token is None:\n",
    "            self.text_tokenizer.pad_token = self.text_tokenizer.eos_token\n",
    "\n",
    "        # CLIP Preprocess\n",
    "        self.clip_model, _, self.clip_preprocess = open_clip.create_model_and_transforms(\n",
    "            'ViT-H-14', pretrained='laion2b_s32b_b79k'\n",
    "        )\n",
    "        self.image_transform = self.clip_preprocess\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        sample_id = row['sample_id']\n",
    "\n",
    "        # Text features\n",
    "        enhanced_text, features = extract_advanced_features(row['catalog_content'])\n",
    "        text_inputs = self.text_tokenizer(\n",
    "            enhanced_text,\n",
    "            max_length=Config.max_text_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Image processing\n",
    "        image_link = row['image_link']\n",
    "        image_filename = Path(image_link).name\n",
    "        image_path = Config.image_folder / image_filename\n",
    "        if not image_path.exists():\n",
    "            download_image_with_retry(image_link, image_path)\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            image = self.image_transform(image)\n",
    "        except Exception:\n",
    "            image = torch.zeros(3, Config.image_size, Config.image_size)\n",
    "\n",
    "        # Numerical features\n",
    "        numerical_features = torch.tensor([\n",
    "            features['value'],\n",
    "            features['num_bullet_points'],\n",
    "            features['text_length'] / 1000.0,\n",
    "            1.0 if features['has_bullet_points'] else 0.0,\n",
    "            1.0 if features['has_description'] else 0.0\n",
    "        ], dtype=torch.float32)\n",
    "\n",
    "        sample = {\n",
    "            'input_ids': text_inputs['input_ids'].squeeze(),\n",
    "            'attention_mask': text_inputs['attention_mask'].squeeze(),\n",
    "            'image': image,\n",
    "            'numerical_features': numerical_features,\n",
    "            'sample_id': sample_id\n",
    "        }\n",
    "\n",
    "        if self.is_training and 'price' in row:\n",
    "            sample['price'] = torch.tensor(float(row['price']), dtype=torch.float32)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba1993cf-53c2-48a5-b828-186472737f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.py\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, os, requests\n",
    "from pathlib import Path\n",
    "from transformers import DebertaV2Tokenizer\n",
    "\n",
    "# -----------------------------\n",
    "# Text Feature Extraction\n",
    "# -----------------------------\n",
    "def extract_advanced_features(text: str):\n",
    "    \"\"\"Extract structured information and create enhanced text.\"\"\"\n",
    "    features = {}\n",
    "\n",
    "    # Basic extractions\n",
    "    item_name_match = re.search(r'Item Name:\\s*(.*?)(?:\\n|$)', text)\n",
    "    value_match = re.search(r'Value:\\s*([0-9.]+)', text)\n",
    "    unit_match = re.search(r'Unit:\\s*(.*?)(?:\\n|$)', text)\n",
    "\n",
    "    features['item_name'] = item_name_match.group(1).strip() if item_name_match else \"\"\n",
    "    features['value'] = float(value_match.group(1)) if value_match else 1.0\n",
    "    features['unit'] = unit_match.group(1).strip() if unit_match else \"\"\n",
    "\n",
    "    # Bullet points\n",
    "    bullet_points = re.findall(r'Bullet Point \\d+:\\s*(.*?)(?:\\n|$)', text)\n",
    "    features['bullet_points'] = bullet_points\n",
    "    features['num_bullet_points'] = len(bullet_points)\n",
    "\n",
    "    # Description\n",
    "    desc_match = re.search(r'Product Description:\\s*(.*?)(?:\\nValue:|$)', text, re.DOTALL)\n",
    "    features['description'] = desc_match.group(1).strip() if desc_match else \"\"\n",
    "\n",
    "    # Enhanced text construction\n",
    "    enhanced_text = f\"Product: {features['item_name']}\"\n",
    "    if bullet_points:\n",
    "        enhanced_text += f\". Key features: {' '.join(bullet_points[:3])}\"\n",
    "    if features['description']:\n",
    "        desc_preview = features['description'][:100] + \"...\" if len(features['description']) > 100 else features['description']\n",
    "        enhanced_text += f\". Description: {desc_preview}\"\n",
    "    if features['value'] > 0:\n",
    "        enhanced_text += f\". Package contains: {features['value']} {features['unit']}\"\n",
    "\n",
    "    # Additional features\n",
    "    features['text_length'] = len(text)\n",
    "    features['has_bullet_points'] = len(bullet_points) > 0\n",
    "    features['has_description'] = len(features['description']) > 0\n",
    "\n",
    "    return enhanced_text, features\n",
    "\n",
    "# -----------------------------\n",
    "# Image Download with Retry\n",
    "# -----------------------------\n",
    "def download_image_with_retry(url, save_path, max_retries=3):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            if os.path.exists(save_path):\n",
    "                return True\n",
    "            response = requests.get(url, timeout=10, stream=True)\n",
    "            if response.status_code == 200:\n",
    "                with open(save_path, 'wb') as f:\n",
    "                    for chunk in response.iter_content(chunk_size=8192):\n",
    "                        f.write(chunk)\n",
    "                return True\n",
    "        except Exception as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                print(f\"❌ Failed to download {url}: {e}\")\n",
    "    return False\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset Class\n",
    "# -----------------------------\n",
    "class AdvancedProductDataset(Dataset):\n",
    "    def __init__(self, dataframe: pd.DataFrame, is_training=True):\n",
    "        self.dataframe = dataframe.reset_index(drop=True)\n",
    "        self.is_training = is_training\n",
    "\n",
    "        # Tokenizer\n",
    "        self.text_tokenizer = DebertaV2Tokenizer.from_pretrained(Config.text_model_name)\n",
    "        if self.text_tokenizer.pad_token is None:\n",
    "            self.text_tokenizer.pad_token = self.text_tokenizer.eos_token\n",
    "\n",
    "        # CLIP Preprocess\n",
    "        self.clip_model, _, self.clip_preprocess = open_clip.create_model_and_transforms(\n",
    "            'ViT-H-14', pretrained='laion2b_s32b_b79k'\n",
    "        )\n",
    "        self.image_transform = self.clip_preprocess\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        sample_id = row['sample_id']\n",
    "\n",
    "        # Text features\n",
    "        enhanced_text, features = extract_advanced_features(row['catalog_content'])\n",
    "        text_inputs = self.text_tokenizer(\n",
    "            enhanced_text,\n",
    "            max_length=Config.max_text_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Image processing\n",
    "        image_link = row['image_link']\n",
    "        image_filename = Path(image_link).name\n",
    "        image_path = Config.image_folder / image_filename\n",
    "        if not image_path.exists():\n",
    "            download_image_with_retry(image_link, image_path)\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            image = self.image_transform(image)\n",
    "        except Exception:\n",
    "            image = torch.zeros(3, Config.image_size, Config.image_size)\n",
    "\n",
    "        # Numerical features\n",
    "        numerical_features = torch.tensor([\n",
    "            features['value'],\n",
    "            features['num_bullet_points'],\n",
    "            features['text_length'] / 1000.0,\n",
    "            1.0 if features['has_bullet_points'] else 0.0,\n",
    "            1.0 if features['has_description'] else 0.0\n",
    "        ], dtype=torch.float32)\n",
    "\n",
    "        sample = {\n",
    "            'input_ids': text_inputs['input_ids'].squeeze(),\n",
    "            'attention_mask': text_inputs['attention_mask'].squeeze(),\n",
    "            'image': image,\n",
    "            'numerical_features': numerical_features,\n",
    "            'sample_id': sample_id\n",
    "        }\n",
    "\n",
    "        if self.is_training and 'price' in row:\n",
    "            sample['price'] = torch.tensor(float(row['price']), dtype=torch.float32)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2001396-28c0-4844-ae98-72369c71ef1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoModel, AutoModelForImageClassification\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "\n",
    "# ---- Optimized Model Definition ----\n",
    "class AdvancedProductPricePredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Text encoder (DeBERTa or other transformer model)\n",
    "        self.text_encoder = AutoModel.from_pretrained(\n",
    "            Config.text_model_name,\n",
    "            torch_dtype=torch.float16 if Config.use_8bit else torch.float32,\n",
    "        )\n",
    "\n",
    "        if Config.use_8bit or Config.use_4bit:\n",
    "            self.text_encoder = prepare_model_for_kbit_training(self.text_encoder)\n",
    "\n",
    "        # Apply LoRA (Low-Rank Adaptation)\n",
    "        lora_config = LoraConfig(\n",
    "            r=Config.lora_r,\n",
    "            lora_alpha=Config.lora_alpha,\n",
    "            target_modules=[\"query_proj\", \"value_proj\", \"key_proj\", \"dense\"],\n",
    "            lora_dropout=Config.lora_dropout,\n",
    "            bias=\"none\",\n",
    "            task_type=TaskType.FEATURE_EXTRACTION\n",
    "        )\n",
    "        self.text_encoder = get_peft_model(self.text_encoder, lora_config)\n",
    "\n",
    "        # Vision encoder (CLIP model for image classification)\n",
    "        self.vision_encoder = AutoModelForImageClassification.from_pretrained(\n",
    "            Config.vision_model_name,\n",
    "            torch_dtype=torch.float16 if Config.use_8bit else torch.float32,\n",
    "        )\n",
    "\n",
    "        # Extract feature dimension from the vision model output\n",
    "        vision_feature_dim = self.vision_encoder.vision_model.config.hidden_size\n",
    "\n",
    "        # Fusion layer to combine text, vision, and numeric features\n",
    "        self.fusion_layer = nn.Sequential(\n",
    "            nn.Linear(self.text_encoder.config.hidden_size + vision_feature_dim + 5, Config.fusion_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(Config.dropout),\n",
    "            nn.Linear(Config.fusion_hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, image, numerical_features):\n",
    "        # Text embeddings\n",
    "        text_emb = self.text_encoder(input_ids, attention_mask=attention_mask).last_hidden_state.mean(dim=1)\n",
    "\n",
    "        # Vision embeddings (use CLIP's `vision_model` to extract features)\n",
    "        vision_output = self.vision_encoder.vision_model(image)\n",
    "        image_emb = vision_output.last_hidden_state.mean(dim=1)  # Assuming last_hidden_state is the output\n",
    "\n",
    "        # Combine text, image, and numeric features\n",
    "        combined = torch.cat([text_emb, image_emb, numerical_features], dim=1)\n",
    "\n",
    "        # Price prediction\n",
    "        price_pred = self.fusion_layer(combined)\n",
    "        return price_pred\n",
    "\n",
    "# ---- Optimizer ----\n",
    "def get_optimizer(model):\n",
    "    return optim.AdamW(model.parameters(), lr=Config.learning_rate, weight_decay=Config.weight_decay)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2aef64c1-1e16-4d61-8f94-c13097660965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 75000 training samples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "109abf83099548d6b6bdcf7ac5ca4fa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a86328f2cab467ab584f1635032c586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dfaf928091a4cad88c3d8ea16b7ab27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/580 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85cc87dfadf54d28b9232f65e8e86ac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "open_clip_model.safetensors:   0%|          | 0.00/3.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 256 worker processes in total. Our suggested max number of worker in current system is 16, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f60c5e886724b0cab2505ca37c25a46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/874M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecdec5b4d4344da7b342bfafaaad63e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/874M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "997e1973887e491d81d78d217f04c09a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d22b1a5fb3f949cbab1e9546773f1aca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CLIPForImageClassification were not initialized from the model checkpoint at laion/CLIP-ViT-H-14-laion2B-s32B-b79K and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 256 worker processes in total. Our suggested max number of worker in current system is 16, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "Epoch 1/10:   0%|          | 0/2110 [00:00<?, ?it/s]/tmp/ipykernel_6300/1858105005.py:53: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.float16):\n",
      "Epoch 1/10: 100%|██████████| 2110/2110 [19:32<00:00,  1.80it/s]\n",
      "/tmp/ipykernel_6300/1858105005.py:76: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.float16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss=12.4310, Train SMAPE=1.1539, Val Loss=10.9667, Val SMAPE=1.0154\n",
      "✅ Saved best model at epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 2110/2110 [19:28<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss=10.0399, Train SMAPE=0.9637, Val Loss=10.2959, Val SMAPE=0.9571\n",
      "✅ Saved best model at epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 2110/2110 [19:26<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss=11.5163, Train SMAPE=1.0846, Val Loss=11.5541, Val SMAPE=1.0797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 2110/2110 [19:26<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss=9.8360, Train SMAPE=0.9605, Val Loss=10.3184, Val SMAPE=0.9794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 2110/2110 [19:28<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss=8.7056, Train SMAPE=0.8770, Val Loss=9.6513, Val SMAPE=0.9048\n",
      "✅ Saved best model at epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 2110/2110 [19:30<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss=7.9920, Train SMAPE=0.8248, Val Loss=9.4989, Val SMAPE=0.9010\n",
      "✅ Saved best model at epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 2110/2110 [19:30<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss=9.6425, Train SMAPE=0.9667, Val Loss=10.5129, Val SMAPE=0.9815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 2110/2110 [19:28<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss=8.9787, Train SMAPE=0.9034, Val Loss=10.6892, Val SMAPE=0.9664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10:   0%|          | 0/2110 [00:00<?, ?it/s]Exception ignored in: <function _releaseLock at 0x7ca72a7239c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/logging/__init__.py\", line 243, in _releaseLock\n",
      "    def _releaseLock():\n",
      "    \n",
      "KeyboardInterrupt: \n",
      "Epoch 9/10:   0%|          | 0/2110 [00:31<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 194470, 194471, 194472, 194473, 194474, 194475, 194476, 194477, 194478, 194479, 194480, 194481, 194502, 194516, 194621, 194733, 194841, 194847, 194848, 194849, 194850, 194851, 194852, 194853, 194854, 194855, 194856, 194857, 194858, 194859, 194860, 194861, 194862, 194863, 194864, 194865, 194866, 194867, 194868, 194869, 194870, 194871, 194872, 194873, 194874, 194875, 194876, 194877, 194878, 194879, 194880, 194881, 194882, 194883, 194885, 194886, 194887, 194888, 194889, 194890, 194891, 194892, 194893, 194894, 194895, 194896, 194897, 194898, 194899, 194900, 194901, 194902, 194903, 194904, 194905, 194931, 194961, 195068, 195179, 195270, 195271, 195272, 195273, 195274, 195275, 195276, 195277, 195278, 195279, 195280, 195281, 195282, 195283, 195284, 195285, 195286, 195287, 195288, 195289, 195290, 195291, 195292, 195293, 195294, 195295, 195296, 195297, 195298, 195299, 195300, 195301, 195302, 195303, 195304, 195305, 195306, 195307, 195308, 195309, 195310, 195311, 195312, 195313, 195314, 195315, 195316, 195317, 195318, 195319, 195320, 195321, 195322, 195323, 195324, 195325, 195326, 195327, 195353, 195433, 195542) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1285\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1284\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1285\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1286\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/queue.py:179\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_empty\u001b[38;5;241m.\u001b[39mwait(remaining)\n",
      "\u001b[0;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 161\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🎯 Final model saved as final_model.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 161\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 147\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    144\u001b[0m model \u001b[38;5;241m=\u001b[39m AdvancedProductPricePredictor()\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# ✅ Train\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbest_model.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m# ✅ Save final model\u001b[39;00m\n\u001b[1;32m    156\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(trained_model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_model.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 50\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, checkpoint_path, resume_checkpoint)\u001b[0m\n\u001b[1;32m     47\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     48\u001b[0m total_smape \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 50\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEpoch \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mamp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautocast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/accelerate/data_loader.py:567\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/utils/data/dataloader.py:734\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 734\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    740\u001b[0m ):\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1492\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data, worker_id)\n\u001b[1;32m   1491\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1492\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1494\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1495\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1444\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1442\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1443\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1444\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1445\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1446\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1298\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1297\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[0;32m-> 1298\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1299\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1300\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[1;32m   1302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 194470, 194471, 194472, 194473, 194474, 194475, 194476, 194477, 194478, 194479, 194480, 194481, 194502, 194516, 194621, 194733, 194841, 194847, 194848, 194849, 194850, 194851, 194852, 194853, 194854, 194855, 194856, 194857, 194858, 194859, 194860, 194861, 194862, 194863, 194864, 194865, 194866, 194867, 194868, 194869, 194870, 194871, 194872, 194873, 194874, 194875, 194876, 194877, 194878, 194879, 194880, 194881, 194882, 194883, 194885, 194886, 194887, 194888, 194889, 194890, 194891, 194892, 194893, 194894, 194895, 194896, 194897, 194898, 194899, 194900, 194901, 194902, 194903, 194904, 194905, 194931, 194961, 195068, 195179, 195270, 195271, 195272, 195273, 195274, 195275, 195276, 195277, 195278, 195279, 195280, 195281, 195282, 195283, 195284, 195285, 195286, 195287, 195288, 195289, 195290, 195291, 195292, 195293, 195294, 195295, 195296, 195297, 195298, 195299, 195300, 195301, 195302, 195303, 195304, 195305, 195306, 195307, 195308, 195309, 195310, 195311, 195312, 195313, 195314, 195315, 195316, 195317, 195318, 195319, 195320, 195321, 195322, 195323, 195324, 195325, 195326, 195327, 195353, 195433, 195542) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from accelerate import Accelerator\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ===============================\n",
    "# SMAPE Metric\n",
    "# ===============================\n",
    "def smape(y_true, y_pred):\n",
    "    \"\"\"Symmetric Mean Absolute Percentage Error\"\"\"\n",
    "    diff = torch.abs(y_true - y_pred)\n",
    "    denominator = (torch.abs(y_true) + torch.abs(y_pred)) / 2.0\n",
    "    return torch.mean(2.0 * diff / (denominator + 1e-8)).item()\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# Training Function\n",
    "# ===============================\n",
    "def train_model(model, train_loader, val_loader, checkpoint_path='best_model.pth', resume_checkpoint=False):\n",
    "    accelerator = Accelerator(mixed_precision='no')\n",
    "    optimizer = get_optimizer(model)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer, T_0=len(train_loader) * 2, T_mult=2\n",
    "    )\n",
    "\n",
    "    model, optimizer, train_loader, val_loader = accelerator.prepare(model, optimizer, train_loader, val_loader)\n",
    "    loss_fn = torch.nn.HuberLoss()\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 0\n",
    "\n",
    "    # ====== Load checkpoint if exists ======\n",
    "    if resume_checkpoint and os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        best_val_loss = checkpoint['best_val_loss']\n",
    "        print(\"✅ Loaded checkpoint from\", checkpoint_path)\n",
    "\n",
    "    # ====== Training Loop ======\n",
    "    for epoch in range(Config.num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        total_smape = 0.0\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{Config.num_epochs}\"):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "                preds = model(\n",
    "                    batch['input_ids'], batch['attention_mask'], batch['image'], batch['numerical_features']\n",
    "                )\n",
    "                loss = loss_fn(preds.squeeze(), batch['price'])\n",
    "\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_smape += smape(batch['price'], preds.squeeze())\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        avg_train_smape = total_smape / len(train_loader)\n",
    "\n",
    "        # ====== Validation ======\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_smape = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "                    preds = model(\n",
    "                        batch['input_ids'], batch['attention_mask'], batch['image'], batch['numerical_features']\n",
    "                    )\n",
    "                    loss = loss_fn(preds.squeeze(), batch['price'])\n",
    "                    val_loss += loss.item()\n",
    "                    val_smape += smape(batch['price'], preds.squeeze())\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        avg_val_smape = val_smape / len(val_loader)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}: \"\n",
    "            f\"Train Loss={avg_train_loss:.4f}, Train SMAPE={avg_train_smape:.4f}, \"\n",
    "            f\"Val Loss={val_loss:.4f}, Val SMAPE={avg_val_smape:.4f}\"\n",
    "        )\n",
    "\n",
    "        # ====== Save Best Checkpoint ======\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience = 0\n",
    "            accelerator.wait_for_everyone()\n",
    "            unwrapped_model = accelerator.unwrap_model(model)\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': unwrapped_model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_val_loss': best_val_loss,\n",
    "            }, checkpoint_path)\n",
    "            print(f\"✅ Saved best model at epoch {epoch+1}\")\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= 5:\n",
    "                print(\"⏹️ Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    return accelerator.unwrap_model(model)\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# Main Function\n",
    "# ===============================\n",
    "def main():\n",
    "    # ✅ Load only the training CSV\n",
    "    full_df = pd.read_csv(Config.train_csv)\n",
    "    print(f\"Loaded {len(full_df)} training samples.\")\n",
    "\n",
    "    # ✅ Split 90% train, 10% validation\n",
    "    val_size = int(0.1 * len(full_df))\n",
    "    train_size = len(full_df) - val_size\n",
    "    train_df, val_df = random_split(full_df, [train_size, val_size])\n",
    "\n",
    "    # Convert Subsets back to DataFrames\n",
    "    train_df = full_df.iloc[train_df.indices]\n",
    "    val_df = full_df.iloc[val_df.indices]\n",
    "\n",
    "    # ✅ Create datasets\n",
    "    train_ds = AdvancedProductDataset(train_df, is_training=True)\n",
    "    val_ds = AdvancedProductDataset(val_df, is_training=True)  # val has price → keep is_training=True\n",
    "\n",
    "    # ✅ DataLoaders\n",
    "    train_loader = DataLoader(train_ds, batch_size=Config.batch_size, shuffle=True,\n",
    "                              num_workers=Config.num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=Config.batch_size, shuffle=False,\n",
    "                            num_workers=Config.num_workers, pin_memory=True)\n",
    "\n",
    "    # ✅ Initialize model\n",
    "    model = AdvancedProductPricePredictor()\n",
    "\n",
    "    # ✅ Train\n",
    "    trained_model = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        checkpoint_path=\"best_model.pth\",\n",
    "        resume_checkpoint=True\n",
    "    )\n",
    "\n",
    "    # ✅ Save final model\n",
    "    torch.save(trained_model.state_dict(), \"final_model.pth\")\n",
    "    print(\"🎯 Final model saved as final_model.pth\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "779330ec-917e-4451-9a4f-30d098a69e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CLIPForImageClassification were not initialized from the model checkpoint at laion/CLIP-ViT-H-14-laion2B-s32B-b79K and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Predicting prices: 100%|██████████| 2344/2344 [25:58<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Inference complete. Saved predictions to test_out.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def run_inference(model_path=\"best_model.pth\", output_csv=\"test_out.csv\"):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = AdvancedProductPricePredictor()\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device)['model_state_dict'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Load the test data\n",
    "    test_df = pd.read_csv(Config.test_csv)\n",
    "    test_ds = AdvancedProductDataset(test_df, is_training=False)\n",
    "    test_loader = DataLoader(test_ds, batch_size=Config.batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "    preds, ids = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Predicting prices\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            image = batch['image'].to(device)\n",
    "            num_feat = batch['numerical_features'].to(device)\n",
    "\n",
    "            # Perform inference (predict price)\n",
    "            price = model(input_ids, attention_mask, image, num_feat)\n",
    "            \n",
    "            # Convert predictions from tensor to float\n",
    "            preds.extend(price.squeeze().cpu().numpy())\n",
    "\n",
    "            # Extract sample IDs (ensure these are the correct IDs for the batch)\n",
    "            ids.extend(batch['sample_id'].cpu().numpy())  # Use .cpu() if needed\n",
    "\n",
    "    # Ensure predictions are non-negative (or apply any other logic)\n",
    "    preds = np.maximum(preds, 0.1)  # Ensures the minimum price is 0.1\n",
    "\n",
    "    # Create a DataFrame for the output\n",
    "    output_df = pd.DataFrame({'sample_id': ids, 'price': preds})\n",
    "\n",
    "    # Save the predictions to a CSV file\n",
    "    output_df.to_csv(output_csv, index=False)\n",
    "    print(f\"✅ Inference complete. Saved predictions to {output_csv}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_inference()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22991600-7c04-40e1-abf2-328c45c6fe93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
